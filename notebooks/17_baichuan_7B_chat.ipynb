{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up tokenizer\n",
      "set up AutoModelForCausalLM\n",
      "GenerationConfig\n"
     ]
    }
   ],
   "source": [
    "print(\"set up tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "print(\"set up AutoModelForCausalLM\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "print(\"GenerationConfig\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", max_new_tokens=1000, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大家好，作为一个大语言模型，是由百川智能的工程师们开发创造，我被设计用于和人类进行自然交流、解答问题、协助创作，帮助大众轻松、普惠的获得世界知识和专业服务。如果你有任何问题，可以随时向我提问\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"自我介绍\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a language model assistant.I was developed by the AI technology of deep learning and natural language processing, capable of understanding and answering questions in English.My main capabilities include: 1. Understanding and answering questions: I can understand and answer questions related to various fields, such as science, math, history, culture, etc.\n",
      "2. Information search: I can help you find information through search engines or other information resources.\n",
      "3. Language translation: I can translate text from one language to another.\n",
      "4. Text summarization: I can summarize long texts into short sentences or key points.\n",
      "5. Sentence completion: I can complete sentences based on the input context.\n",
      "6. Dialog system: I can conduct dialog with users through text or voice.\n",
      "7. Other tasks: I can also participate in other tasks such as data entry, document review, etc.\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"Please introduce yourself and your capabilities\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, instructions, model, tokenizer):\n",
    "    # pass\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{instructions}: {text}\"})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Stone pillar\" is a simple and down-to-earth word, not fancy enough, but it can accurately convey our emotions and confidence in the construction of China's science fiction grand skyscraper. Therefore, we use it as the name of this series of original collections.\n",
      "\"Stone pillar\" is a simple and down-to-earth word, not fancy enough, but it can accurately convey our emotions and confidence in the construction of China's science fiction grand skyscraper. Therefore, we use it as the name of this series of original collections.\n"
     ]
    }
   ],
   "source": [
    "CN_TEXT = \"“基石”是个平实的词，不够“炫”，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\"\n",
    "# INSTRUCTIONS = \"Summarize the following in Chinese\"\n",
    "INSTRUCTIONS = \"用中文概括下面一段\"\n",
    "response = inference(CN_TEXT, INSTRUCTIONS, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"基石\"一书以朴实词汇传达对中国科幻繁荣发展的情感与信心，成为这套原创丛书的命名。\n"
     ]
    }
   ],
   "source": [
    "CN_TEXT = \"“基石”是个平实的词，不够“炫”，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\"\n",
    "# INSTRUCTIONS = \"Summarize the following in Chinese\"\n",
    "INSTRUCTIONS = \"用一句话概括下面一段\"\n",
    "response = inference(CN_TEXT, INSTRUCTIONS, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace translate_paragraph function with generic inference function\n",
    "\n",
    "import json\n",
    "\n",
    "def translate_chapter(llm, book_path, chapter_number):\n",
    "    cn_en_translation_instructions = \"\"\n",
    "    # open the file\n",
    "    with open(f\"../data/books/{book_path}/chapters/{chapter_number}.json\", \"r\") as f:\n",
    "        chapter = json.loads(f.read())\n",
    "        translated_paragraphs = []\n",
    "        summaries = []\n",
    "        translated_summaries = []\n",
    "        for paragraph in chapter[\"paragraphs\"]:\n",
    "\n",
    "            # translate cn to en\n",
    "            translated_paragraph = inference(CN_TEXT, INSTRUCTIONS, model, tokenizer)\n",
    "            translated_paragraphs.append(translated_paragraph)\n",
    "\n",
    "            # cn_summary\n",
    "\n",
    "            # en_summary\n",
    "\n",
    "        chapter[\"translated_paragraphs_baichuan2_7b\"] = translated_paragraphs\n",
    "\n",
    "    with open(f\"../data/books/{book_path}/chapters/{chapter_number}.json\", \"w\") as f:\n",
    "        json.dump(chapter, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"translated {len(translated_paragraphs)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
