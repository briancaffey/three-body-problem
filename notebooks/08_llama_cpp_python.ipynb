{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7aa282c-c29b-4576-96f3-ff5e4581d678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from llama_cpp import Llama, Completion, LlamaTokenizer\n",
    "from utils import translate_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4174a1df-eb9a-4756-9de9-a717dcf3bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's translate the text of the first chapter\n",
    "with open(\"../data/books/three_body/chapters/1.json\", \"r\") as f:\n",
    "    chapter_1_json = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ffbaf2-903f-4d82-a6fe-b3aa0a4592b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = chapter_1_json[\"paragraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c41d36-2d3c-4e80-a6da-f70674811b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1041.98 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 512 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 30 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 30/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 3770 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=\"/home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\",\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f4559c-3e1e-4eea-8a79-407447753d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function that can add English translations to our file for a given chapter in a book\n",
    "import json\n",
    "\n",
    "def translate_chapter(llm, book_path, chapter_number):\n",
    "    # open the file\n",
    "    with open(f\"../data/books/{book_path}/chapters/{chapter_number}.json\", \"r\") as f:\n",
    "        chapter = json.loads(f.read())\n",
    "        translated_paragraphs = []\n",
    "        for paragraph in chapter[\"paragraphs\"]:\n",
    "            translated_paragraph = translate_paragraph(paragraph, llm=llm, method=\"completion\")\n",
    "            translated_paragraphs.append(translated_paragraph[\"choices\"][0][\"text\"])\n",
    "\n",
    "        chapter[\"translated_paragraphs\"] = translated_paragraphs\n",
    "\n",
    "    with open(f\"../data/books/{book_path}/chapters/{chapter_number}.json\", \"w\") as f:\n",
    "        json.dump(chapter, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"translated {len(translated_paragraphs)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e10933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 37):\n",
    "    translate_chapter(llm, \"three_body\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470624a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took 446 minutes to run\n",
    "for i in range (1,51):\n",
    "    translate_chapter(llm, \"dark_forest\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,59):\n",
    "    translate_chapter(llm, \"deaths_end\", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
