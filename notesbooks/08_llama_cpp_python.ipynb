{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7aa282c-c29b-4576-96f3-ff5e4581d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama, Completion, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174a1df-eb9a-4756-9de9-a717dcf3bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's translate the text of the first chapter\n",
    "with open(\"../data/books/three_body/chapters/1.json\", \"r\") as f:\n",
    "    chapter_1_json = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c41d36-2d3c-4e80-a6da-f70674811b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1041.98 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 512 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 30 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 30/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 3770 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=\"/home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\",\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9fe719-deb0-4b5e-84b0-61bbeaf111f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_paragraph(text, llm, use_example=None, method=None):\n",
    "    model_path = \"/home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\"\n",
    "    # system_prompt=\"You will be provided with Chinese language text. Please respond with the English translation of the text. 你是翻译者,请把输入的段子翻译成英文。\"\n",
    "    system_prompt=\"Translate from Chinese to English, only reply in English 把中文翻译成英文，只用英文回答问题\"\n",
    "    n_ctx = 4096\n",
    "    llm = Llama(model_path=model_path, n_ctx=n_ctx)\n",
    "\n",
    "    # message components\n",
    "    messages = []\n",
    "\n",
    "    # system prompt\n",
    "    system_prompt = {\"role\": \"system\", \"content\": system_prompt}\n",
    "    messages.append(system_prompt)\n",
    "\n",
    "    # examples to help the completion get a better sense of how to translate\n",
    "    if use_example:\n",
    "        messages.append({\"role\": \"user\", \"content\": \"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"In that forgotten day, its world was turned upside down. The soil flew away and a deep and wide canyon appeared, then the soil flew back and the canyon disappeared, at the end of which appeared a black peak. In fact, such things often happened in this vast domain, where the soil flew away and flew back, then there was a canyon, then a peak, as if to mark each disaster. The brown ants and hundreds of their kind with surviving queens walked towards the direction where the sun set and established a new empire.\"})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    if method == \"completion\":\n",
    "        completion_prompt = f\"\\n\\n### Chinese:\\n${text}\\n\\n### English:\\n\"\n",
    "        stop = [\"\\n\",\"###\"]\n",
    "        response = llm.create_completion(max_tokens=5000,model=model_path, prompt=completion_prompt, stop=stop)\n",
    "        print(response[\"choices\"][0][\"text\"])\n",
    "        return response\n",
    "    else:\n",
    "        response = llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=5000,\n",
    "            model=model_path\n",
    "        )\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82397e82-5a2c-4694-ab15-380557ef5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 512 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown ant has forgotten that this was once its home. To the earth and the newly rising stars, these moments are negligible; but to the ant, they lasted forever.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   594.70 ms\n",
      "llama_print_timings:      sample time =    18.22 ms /    39 runs   (    0.47 ms per token,  2140.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   594.68 ms /    55 tokens (   10.81 ms per token,    92.49 tokens per second)\n",
      "llama_print_timings:        eval time =  3315.26 ms /    38 runs   (   87.24 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =  3990.86 ms\n"
     ]
    }
   ],
   "source": [
    "output = translate_paragraph(\n",
    "    \"褐蚁已经忘记这里曾是它的家园。这段时光对于暮色中的大地和刚刚出现的星星来说短得可以忽略不计，但对于它来说却是漫长的。\",\n",
    "    llm=llm,\n",
    "    method=\"completion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d6b87-0038-453c-977c-f62667ca78b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2103c6c-ef99-4569-8094-bb9ca841af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 512 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown ants continued to climb along a parallel direction with the ground, entering the third ditch. It was a nearly right-angled turn, as follows: “7”. The brown ants did not like this shape; usually, such abrupt and uneven turns mean danger and battle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1148.01 ms\n",
      "llama_print_timings:      sample time =    29.15 ms /    63 runs   (    0.46 ms per token,  2161.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1147.96 ms /   345 tokens (    3.33 ms per token,   300.53 tokens per second)\n",
      "llama_print_timings:        eval time =  5673.53 ms /    62 runs   (   91.51 ms per token,    10.93 tokens per second)\n",
      "llama_print_timings:       total time =  6952.42 ms\n"
     ]
    }
   ],
   "source": [
    "output = translate_paragraph(\n",
    "    \"褐蚁继续沿着与地面平行的方向爬，进入了第三道沟槽，它是一个近似于直角的转弯，是这样的：“7”。它不喜欢这形状，平时，这种不平滑的、突然的转向，往往意味着危险和战斗。\",\n",
    "    llm=llm,\n",
    "    use_example=True\n",
    ")\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985a192-7ede-4b4b-a4e9-1ead253a7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10841958-f6b7-4c99-8e6f-8f412af5b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd801223-9972-442f-a740-ae1f20f927d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 512 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In that forgotten day, its world was transformed. Soil flew away, and a deep and wide valley appeared, then the soil returned and the valley disappeared at the end of it. It was a familiar sight to the vast wilderness: the soil flew away and returned, as well as the valley's appearance and disappearance. A black peak arrived there, like a flag in every disaster. Brown ants with hundreds of their tribe left behind walked towards the direction where the sun sets, and established a new empire.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   791.39 ms\n",
      "llama_print_timings:      sample time =    49.42 ms /   111 runs   (    0.45 ms per token,  2246.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   791.36 ms /   142 tokens (    5.57 ms per token,   179.44 tokens per second)\n",
      "llama_print_timings:        eval time =  9651.34 ms /   110 runs   (   87.74 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time = 10666.27 ms\n"
     ]
    }
   ],
   "source": [
    "output = translate_paragraph(\n",
    "    TEXT,\n",
    "    llm=llm,\n",
    "    method=\"completion\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b20d7c-13f5-4988-94c1-15630d316dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 55296\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 5504\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 512 MB\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In that forgotten day, its world was turned upside down. The soil flew away and a deep and wide canyon appeared, then the soil flew back and the canyon disappeared, at the end of which appeared a black peak. In fact, such things often happened in this vast domain, where the soil flew away and flew back, then there was a canyon, then a peak, as if to mark each disaster. The brown ants and hundreds of their kind with surviving queens walked towards the direction where the sun set and established a new empire.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1117.16 ms\n",
      "llama_print_timings:      sample time =    58.16 ms /   127 runs   (    0.46 ms per token,  2183.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1117.11 ms /   319 tokens (    3.50 ms per token,   285.56 tokens per second)\n",
      "llama_print_timings:        eval time = 11414.08 ms /   126 runs   (   90.59 ms per token,    11.04 tokens per second)\n",
      "llama_print_timings:       total time = 12788.80 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-21a5571b-7b71-4089-af32-6839548c5ab5',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1692018905,\n",
       " 'model': '/home/brian/github/llama.cpp/models/7B/Chinese-Alpaca-2/ggml-model-q4_0.bin',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'In that forgotten day, its world was turned upside down. The soil flew away and a deep and wide canyon appeared, then the soil flew back and the canyon disappeared, at the end of which appeared a black peak. In fact, such things often happened in this vast domain, where the soil flew away and flew back, then there was a canyon, then a peak, as if to mark each disaster. The brown ants and hundreds of their kind with surviving queens walked towards the direction where the sun set and established a new empire.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 319,\n",
       "  'completion_tokens': 127,\n",
       "  'total_tokens': 446}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TEXT=\"破壁人二号（凄凉地笑笑）：“好了，都结束了，那边肯定是无梦的。”三体小说\"\n",
    "\n",
    "output = translate_paragraph(\n",
    "    TEXT,\n",
    "    llm=llm,\n",
    "    use_example=True\n",
    ")\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca24c85-2dbb-487c-ae27-c26941ba120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer(llama=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329386a9-4f92-4135-bba8-953feb415025",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"\n",
    "tokens = tokenizer.encode(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb429d0-613a-4f5a-93f3-37f60e5136a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 30505,\n",
       " 32380,\n",
       " 36812,\n",
       " 36183,\n",
       " 32067,\n",
       " 36162,\n",
       " 30755,\n",
       " 30214,\n",
       " 33592,\n",
       " 32109,\n",
       " 40829,\n",
       " 30743,\n",
       " 30267,\n",
       " 34230,\n",
       " 31181,\n",
       " 32255,\n",
       " 32069,\n",
       " 30214,\n",
       " 34260,\n",
       " 32915,\n",
       " 32018,\n",
       " 31947,\n",
       " 32018,\n",
       " 33162,\n",
       " 30210,\n",
       " 48207,\n",
       " 30214,\n",
       " 32088,\n",
       " 34230,\n",
       " 31181,\n",
       " 32018,\n",
       " 42303,\n",
       " 31788,\n",
       " 31788,\n",
       " 30533,\n",
       " 32255,\n",
       " 33206,\n",
       " 30214,\n",
       " 48207,\n",
       " 35030,\n",
       " 30743,\n",
       " 30214,\n",
       " 30505,\n",
       " 33419,\n",
       " 48207,\n",
       " 30210,\n",
       " 47667,\n",
       " 34260,\n",
       " 35714,\n",
       " 33760,\n",
       " 30210,\n",
       " 35841,\n",
       " 32971,\n",
       " 30267,\n",
       " 32106,\n",
       " 30214,\n",
       " 35200,\n",
       " 31122,\n",
       " 42877,\n",
       " 30210,\n",
       " 39215,\n",
       " 34872,\n",
       " 30429,\n",
       " 30214,\n",
       " 43843,\n",
       " 34826,\n",
       " 32254,\n",
       " 30214,\n",
       " 34230,\n",
       " 31181,\n",
       " 32255,\n",
       " 32069,\n",
       " 32018,\n",
       " 32255,\n",
       " 30742,\n",
       " 30214,\n",
       " 48207,\n",
       " 32143,\n",
       " 32018,\n",
       " 35030,\n",
       " 30214,\n",
       " 32088,\n",
       " 30392,\n",
       " 35841,\n",
       " 32971,\n",
       " 47617,\n",
       " 30214,\n",
       " 40941,\n",
       " 31999,\n",
       " 32827,\n",
       " 35748,\n",
       " 31462,\n",
       " 31656,\n",
       " 30429,\n",
       " 32002,\n",
       " 50121,\n",
       " 30210,\n",
       " 45962,\n",
       " 30267,\n",
       " 52694,\n",
       " 49280,\n",
       " 30503,\n",
       " 37241,\n",
       " 30502,\n",
       " 30980,\n",
       " 30999,\n",
       " 33675,\n",
       " 31762,\n",
       " 30946,\n",
       " 30210,\n",
       " 49280,\n",
       " 30822,\n",
       " 31331,\n",
       " 34109,\n",
       " 32327,\n",
       " 35096,\n",
       " 32729,\n",
       " 35002,\n",
       " 33625,\n",
       " 30874,\n",
       " 30214,\n",
       " 40783,\n",
       " 33179,\n",
       " 36231,\n",
       " 30267]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b73c32-c163-422e-8d19-e3a44089aa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 在 那个 已被 忘 却 的日子 里 ， 它的 世界 颠覆 了 。 泥 土 飞 走 ， 出现了 一条 又 深 又 宽 的 峡谷 ， 然后 泥 土 又 轰 隆 隆 地 飞 回来 ， 峡谷 消失 了 ， 在 原来 峡谷 的 尽头 出现了 一座 黑色 的 孤 峰 。 其实 ， 在这 片 广阔 的 疆 域 上 ， 这种事 常常 发生 ， 泥 土 飞 走 又 飞 回 ， 峡谷 出现 又 消失 ， 然后 是 孤 峰 降临 ， 好像是 给 每次 灾 变 打 上 一个 醒目 的 标记 。 褐 蚁 和 几百 个 同 族 带着 幸 存 的 蚁 后 向 太阳 落 下的 方向 走了 一段 路 ， 建立了 新的 帝国 。 "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    text = tokenizer.decode([token])\n",
    "    print(text, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559f8cf-8406-4d1d-8b80-2925c8d060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=\"字幕：我们再进行第二项证实——伊文斯曾经给你发过一封加密信，但密码变了，他没来得及通知你新的密码就死了，你一直打不开那封信。现在我告诉你密码——CAMEL，就是你毒死金鱼的香烟的牌子。\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
